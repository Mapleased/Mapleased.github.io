<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>个人简历</title>
    <link href="/2025/04/10/%E4%B8%AA%E4%BA%BA%E7%AE%80%E5%8E%86/"/>
    <url>/2025/04/10/%E4%B8%AA%E4%BA%BA%E7%AE%80%E5%8E%86/</url>
    
    <content type="html"><![CDATA[<h1 id="符伟麟的简历"><a href="#符伟麟的简历" class="headerlink" title="符伟麟的简历"></a>符伟麟的简历</h1><p><strong>男 | 30岁 | 13265146234 | <a href="mailto:&#x34;&#x31;&#51;&#54;&#49;&#55;&#54;&#x31;&#51;&#64;&#113;&#113;&#x2e;&#99;&#111;&#x6d;">413617613@qq.com</a></strong><br><strong>9年工作经验 | 求职意向：Java | 期望薪资：20-22K | 期望城市：广州</strong></p><h2 id="个人优势"><a href="#个人优势" class="headerlink" title="个人优势"></a>个人优势</h2><ul><li>4年大数据 Hadoop 体系知识图谱研发经验</li><li>熟悉企业服务总线 (ESB) 和主数据管理 (MDM) 解决方案</li><li>具备从 0 到 1 搭建微服务架构和大数据中台的能力</li><li>对业务拆解、架构设计、技术实现、程序优化有丰富经验</li><li>需求分析敏锐，能快速制定高效的解决方案</li><li>熟悉 AI 应用编程、RAG模型微调、SpringAI方案的实用化</li><li>学习能力强，可根据公司需求调整职业发展方向</li></ul><hr><h2 id="工作经历"><a href="#工作经历" class="headerlink" title="工作经历"></a>工作经历</h2><h3 id="个人项目协助-2024-01-2024-03"><a href="#个人项目协助-2024-01-2024-03" class="headerlink" title="个人项目协助 (2024.01 - 2024.03)"></a>个人项目协助 <em>(2024.01 - 2024.03)</em></h3><p><strong>职位：AI应用开发</strong></p><h4 id="工作内容："><a href="#工作内容：" class="headerlink" title="工作内容："></a>工作内容：</h4><ul><li>对AI大模型相关技术栈进行调研，并实现应用落地</li><li>构建SpringAI体系模版，使得应用获得接口与AI联动的能力</li><li>对项目进行性能调优及代码审查</li></ul><hr><h3 id="润建股份有限公司广东分公司-2022-12-2024-08"><a href="#润建股份有限公司广东分公司-2022-12-2024-08" class="headerlink" title="润建股份有限公司广东分公司 (2022.12 - 2024.08)"></a>润建股份有限公司广东分公司 <em>(2022.12 - 2024.08)</em></h3><p><strong>职位：高级Java 开发</strong></p><h4 id="工作内容：-1"><a href="#工作内容：-1" class="headerlink" title="工作内容："></a>工作内容：</h4><ul><li>负责公司大数据架构建设，从无到有搭建数据平台</li><li>设计、搭建、维护 ESB 运行平台及 MDM 主数据管理平台</li><li>参与数字广东的《赋码平台》开发及架构优化</li><li>负责公司核心项目方案设计、技术支持和需求评审</li><li>主导赋码管理系统、赋码官网、政务服务码、领导行程、督办落实等多个子系统，获客户一致好评</li></ul><hr><h3 id="泽恩科技有限公司-2018-05-2022-11"><a href="#泽恩科技有限公司-2018-05-2022-11" class="headerlink" title="泽恩科技有限公司 (2018.05 - 2022.11)"></a>泽恩科技有限公司 <em>(2018.05 - 2022.11)</em></h3><p><strong>职位：Java 开发</strong></p><h4 id="工作内容：-2"><a href="#工作内容：-2" class="headerlink" title="工作内容："></a>工作内容：</h4><ul><li>参与项目招投标后的全生命周期，包括需求对接、环境搭建、系统设计、核心模块开发、售后运维等</li><li>拓展公司的技术栈，提升大数据能力</li><li>自主搭建大数据集群，并开发知识图谱生态系统，为公司赢得多个项目</li><li>开发可视化大屏和数据仓储管理模块，提高项目竞争力</li></ul><hr><h3 id="广州升日科技有限公司-2015-08-2018-04"><a href="#广州升日科技有限公司-2015-08-2018-04" class="headerlink" title="广州升日科技有限公司 (2015.08 - 2018.04)"></a>广州升日科技有限公司 <em>(2015.08 - 2018.04)</em></h3><p><strong>职位：Java 开发</strong></p><h4 id="工作内容：-3"><a href="#工作内容：-3" class="headerlink" title="工作内容："></a>工作内容：</h4><ul><li>负责与乙方对接需求，设计数据库，开发并维护后台管理系统</li><li>编写项目文档，优化框架，负责 UI 设计</li></ul><hr><h2 id="项目经验"><a href="#项目经验" class="headerlink" title="项目经验"></a>项目经验</h2><h3 id="SpringAI智能客服-2025-02-2025-03"><a href="#SpringAI智能客服-2025-02-2025-03" class="headerlink" title="SpringAI智能客服 (2025.02 - 2025.03)"></a>SpringAI智能客服 <em>(2025.02 - 2025.03)</em></h3><p><strong>角色：主要负责人</strong></p><ul><li>对大模型相关技术栈进行调研，并实现应用落地</li><li>结合SpringAI实现基于大模型和Java应用的接口互动</li><li>使用LLM对模型进行微调，优化向量</li><li>构建智能客服生态体系，在于AI语音交互的同时完成订单业务</li></ul><hr><h3 id="赋码管理平台（数字广东）-2022-05-2024-07"><a href="#赋码管理平台（数字广东）-2022-05-2024-07" class="headerlink" title="赋码管理平台（数字广东） (2022.05 - 2024.07)"></a>赋码管理平台（数字广东） <em>(2022.05 - 2024.07)</em></h3><p><strong>角色：主要开发人员</strong></p><ul><li>负责开发二维码发放与管理平台，支撑广东政务一码通用</li><li>参与国家政务服务码和粤治慧的对接</li><li>通过 H5+动态表单+二维码，支持快速搭建应用</li><li>作为广东政务服务码指定发码平台，获得客户认可</li></ul><hr><h3 id="MDM-主数据管理平台-2023-01-2023-07"><a href="#MDM-主数据管理平台-2023-01-2023-07" class="headerlink" title="MDM 主数据管理平台 (2023.01 - 2023.07)"></a>MDM 主数据管理平台 <em>(2023.01 - 2023.07)</em></h3><p><strong>角色：主要负责人</strong></p><ul><li>采用 Hadoop、Spark、Hive 搭建大数据框架</li><li>整合 DolphinScheduler 和 Zeppelin，实现数据分析、清洗、建模</li><li>结合 ESB 运行平台，实现业务数据订阅</li><li>搭建完整的数据管理平台，优化跨平台数据整合</li><li>提供详细文档支持，提高团队开发效率</li></ul><hr><h3 id="ESB-运行平台-2022-12-2023-07"><a href="#ESB-运行平台-2022-12-2023-07" class="headerlink" title="ESB 运行平台 (2022.12 - 2023.07)"></a>ESB 运行平台 <em>(2022.12 - 2023.07)</em></h3><p><strong>角色：主要开发人员</strong></p><ul><li>基于 WSO2 EI 开发企业服务总线 (ESB)</li><li>提供 API 订阅、监控、授权、统计分析</li><li>适配 RESTful API 和 WebService 进行协议转换</li><li>统一管理跨平台数据接口，降低联调成本</li></ul><hr><h3 id="知识图谱数据管理平台-2019-04-2022-11"><a href="#知识图谱数据管理平台-2019-04-2022-11" class="headerlink" title="知识图谱数据管理平台 (2019.04 - 2022.11)"></a>知识图谱数据管理平台 <em>(2019.04 - 2022.11)</em></h3><p><strong>角色：主要负责人</strong></p><ul><li>自主研发公司大数据技术栈，搭建大数据集群</li><li>接入 JanusGraph，开发知识图谱系统</li><li>负责业务培训，帮助合作伙伴完成数据建模</li></ul><hr><h2 id="技术栈"><a href="#技术栈" class="headerlink" title="技术栈"></a>技术栈</h2><ul><li><strong>后端开发</strong>：Java, Spring Boot, Spring Cloud, MyBatis, Redis, Kafka</li><li><strong>大数据</strong>：Hadoop, Spark, Hive, HBase, JanusGraph, ElasticSearch</li><li><strong>微服务架构</strong>：Docker, Kubernetes, WSO2, Nacos</li><li><strong>数据库</strong>：MySQL, PostgreSQL, OpenGauss, Oracle</li><li><strong>自动化运维</strong>：Jenkins, K8S, Shell, Actions</li></ul><hr><h2 id="教育背景"><a href="#教育背景" class="headerlink" title="教育背景"></a>教育背景</h2><p><strong>广东东软学院</strong> <em>(2013 - 2016)</em><br><strong>专业：计算机应用技术（大专）</strong></p><pre><code class="hljs"></code></pre>]]></content>
    
    
    <categories>
      
      <category>简历</category>
      
      <category>个人介绍</category>
      
    </categories>
    
    
    <tags>
      
      <tag>SpringAI</tag>
      
      <tag>RAG</tag>
      
      <tag>Java</tag>
      
      <tag>架构师</tag>
      
      <tag>大数据</tag>
      
      <tag>Flink</tag>
      
      <tag>Kafka</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AI智能应用可以实用化到什么程度</title>
    <link href="/2025/03/06/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%A6%82%E4%BD%95%E6%90%AD%E5%BB%BAAI%E6%99%BA%E8%83%BD%E5%BA%94%E7%94%A8/"/>
    <url>/2025/03/06/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%A6%82%E4%BD%95%E6%90%AD%E5%BB%BAAI%E6%99%BA%E8%83%BD%E5%BA%94%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<h1 id="基于如何使用SpringAI的思考"><a href="#基于如何使用SpringAI的思考" class="headerlink" title="基于如何使用SpringAI的思考"></a>基于如何使用SpringAI的思考</h1><p>根据一段时间对AI相关技术栈的研究，进行了一系列的技术选型，决定搭建一套AI应用技术解决方案。</p><p>一个优秀的技术解决方案，需要有易于变现的能力。谈论到AI，最先想到的都是聊天机器人和虚拟助手（如智能客服），需要检索产品信息和FAQ的场景，内容生成和总结类服务（法律医疗教育行业），企业内部知识库和客户关系管理（CRM）系统。那如何设计一个不依赖AI的技术解决方案呢？他可以用于改变当前的开发模式，保证更高效的性能和更大的适用性。基于这个思考，那么我的AI服务必须满足以下所有条件：</p><ul><li>此方案不能局限于行业</li><li>对比传统的开发编程具有优势性</li><li>优化用户体验、提高业务办理效率</li><li>具有智能化</li></ul><h3 id="举例一个最普适的场景，项目管理系统："><a href="#举例一个最普适的场景，项目管理系统：" class="headerlink" title="举例一个最普适的场景，项目管理系统："></a>举例一个最普适的场景，项目管理系统：</h3><h4 id="业务知识"><a href="#业务知识" class="headerlink" title="业务知识"></a>业务知识</h4><ul><li>存在人员管理模块，部门管理模块，项目管理模块。</li><li>人员与部门强相关。</li><li>部门与项目强相关，项目只由指定的部门负责。（项目是部门人员接回来的）</li><li>每个项目都由相关的人员负责（并非同个部门就需要处理所有该部门的项目）</li><li>项目中存在项目人员中间表绑定人员与项目的关系，方便人员查看自己的项目</li><li>当人员发生部门变更，则需要解除在原部门绑定的所有项目关联，重新绑定新的项目</li></ul><h4 id="补充提示词"><a href="#补充提示词" class="headerlink" title="补充提示词"></a>补充提示词</h4><ul><li>当询问到变更部门相关信息时，请向用户确认用户的工号，需要变更的部门，变更后需要接收的项目</li><li>当确认了用户的工号后，需要先查询客户的基本信息，变更前的部门，变更前负责的项目，变更后的部门，变更后需要负责的项目，并进行回显，要求用户确认变更</li><li>当确认变更后，调用接口执行业务变更</li><li>在确认变更阶段发生了补充信息，请重新查询新的信息返回并再次确认变更</li><li>在业务处理时，需要先返回业务相关的核心信息，进行信息确认，确认后再执行</li><li>人员的确认信息为 user.userNo(工号)、user.userName(用户名)</li><li>部门的确认信息为 dept.id(部门ID)、dept.deptName(部门名称)</li><li>项目的确认信息为 project.id(部门ID)、dept.projectName(部门名称)</li><li>如果存在人员、项目或者部门同名的情况下，需要验证制ID</li><li>确认信息时使用 user.userNo(工号)， 处理业务时查询出user.id(用户ID)用于业务流转</li></ul><hr><h4 id="部门调动的交互方式（预实现的接口）"><a href="#部门调动的交互方式（预实现的接口）" class="headerlink" title="部门调动的交互方式（预实现的接口）"></a>部门调动的交互方式（预实现的接口）</h4><ul><li>打开项目管理页面 </li><li>根据用户查询该用户绑定的相关的项目列表 getProjectList(user.id)</li><li>进入项目的管理页面 getProject(project.id)</li><li>进入人员编辑页面 getUserByProjectId(project.id)</li><li>移除该用户 delUserByProject(user.id, project.id)</li><li>打开部门管理页面 getDept()</li><li>在部门管理页面根据部门查询人员 getUserByDeptId(dept.id)</li><li>部门管理页面将该人员移除 delUserByDept(user.id, dept.id)</li><li>部门管理页面对新的部门添加该人员 addUserByDept(user.id, dept.id)</li><li>打开项目管理页面 getProjectList(project.projectName)</li><li>逐个点进该人员需要加入的项目 getProject(project.id)</li><li>进入 项目的管理-人员编辑页面 getUserByProjectId(project.id)</li><li>添加该用户 addUserByProject(user.id, project.id)</li></ul><h4 id="业务明知项分析"><a href="#业务明知项分析" class="headerlink" title="业务明知项分析"></a>业务明知项分析</h4><p>在这个过程中，系统操作人员需要明知的信息项：<br>需要被变更的人员的工号或ID (避免重名)<br>需要变更到的新部门ID<br>员工变更部门后需要加入的项目列表</p><hr><h4 id="SpringAI的应用"><a href="#SpringAI的应用" class="headerlink" title="SpringAI的应用"></a>SpringAI的应用</h4><p>此时我们可以简化这个流程，定义一个SpringAI监听器，同于监听以上的所有接口。<br>将<strong><strong>业务知识</strong></strong>和<strong><strong>补充提示词</strong></strong>作为该监听的提示词输入到模型中。<br>由前端开发会话窗口与后端监听器接口进行绑定。<br>由前端发起会话，SpringAI根据会话内容自动调用后端监听的接口列表实现业务。</p><h4 id="SpringAI的执行流程"><a href="#SpringAI的执行流程" class="headerlink" title="SpringAI的执行流程"></a>SpringAI的执行流程</h4><p>向AI发出指令：部门变更</p><p>AI或尝试询问需要变更的用户的工号，需要变更的部门，变更后需要接受的项目</p><p>确认信息后，AI会查询相关信息并进行确认</p><p>当信息不完全正确时，只需要补充待修改的部分，AI会根据需求进行确认信息修改并回显</p><p>确认变更后AI会自动调用接口依次执行业务逻辑</p><p>交互方式默认为对话框，可以优化为下拉框或按钮的联动</p><p>在补充了监听器后，除了变更部门这种复杂业务，只要涉及到简单的CRUD，AI都能从会话框快速回答或者交互按钮，实现一个页面完成所有业务操作</p><p>可以以联动数据表格的形式无跳转完成业务处理，也可以实现左侧嵌入式跳转变更表格信息</p><h3 id="SpringAI做了什么？"><a href="#SpringAI做了什么？" class="headerlink" title="SpringAI做了什么？"></a>SpringAI做了什么？</h3><p>SpringAI只是优化了业务的交互流程，如果涉及到工作流和OA等需要严格按步骤执行的事务，只需要将必须严谨执行的流程投喂给AI，他就会在满足前置条件的前提下，帮你调用接口进行数据交互并完成业务办理。本质上，我们仍然需要编写基础的Java接口，并没有办法用SpringAI去替代Java的应用开发。</p><h3 id="如何进一步优化"><a href="#如何进一步优化" class="headerlink" title="如何进一步优化"></a>如何进一步优化</h3><p>此时，应该已经了解了SpringAI可以用来做什么，能做到什么程度。那么，应该怎么将其实用化呢？<br>我们可以构建一个SpringAI工厂。<br>首先搭建一个可视化的SpringAI应用后台，用于维护SpringAI的事例监听器，现有的接口清单。</p><ul><li>指定一个监听器的权限命名 （用于前端对会话窗口进行绑定）</li><li>配置该监听器相关的接口 （SpringAI 可以代为执行的接口清单）</li><li>配置监听器的业务知识 （业务流程相关知识，仅用于参考）</li><li>执行监听器时需要严格执行的提示词（配置必须匹配的规则）</li><li>绑定外部知识引用模型或文件 （可选项，涉及到FQA业务时，补充RAG调用，减少模型的幻读）</li></ul><p>项目启动时，根据SpringAI进行自动化构建多个应用，进行执行。</p><p>前段只需要根据监听器的权限命名，用于绑定不同的接口入口，即可 左侧图表右侧对话窗口 或者 完全对话 实现快捷业务办理。</p><p>SpringAI支持纯会话办理业务，补充分屏仅为了提供优化确保流程正确。在配置了工作流的前提下，可以完全会话的形式办理业务，若不使用工作流，需要页面辅助以确保更方便更少的交互完成与AI的互动，可以确保在会话后是否完成即使变更再进行下一步操作。</p><h3 id="结合RAG适用的应用场景："><a href="#结合RAG适用的应用场景：" class="headerlink" title="结合RAG适用的应用场景："></a>结合RAG适用的应用场景：</h3><p>SpringAI + RAG + 大模型的解决方案 是一种结合SpringAI框架、检索增强生成（RAG）技术和大型语言模型的系统，适合需要AI从外部知识源获取信息的Java应用程序。<br>以下是其主要适用场景：</p><ul><li>类似于智能客服，解决与人交互问题的<br>聊天机器人和虚拟助手：如客户支持系统或内部知识库，AI需要根据公司文档或用户数据提供回答。例如，电商网站上的自动客服机器人可以检索产品信息和FAQ来回答问题。</li><li>内容生成和总结：<br>如根据最新数据生成新闻摘要或报告，AI可以从实时数据中提取信息，确保内容准确。</li><li>数据分析和洞察：<br>如市场趋势分析或客户行为研究，AI通过检索相关数据集提供见解。</li><li>合规性和监管应用：<br>如法律文档合规检查，AI可以检索最新法律法规，确保响应符合要求。</li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>大模型的现状与提示词</title>
    <link href="/2025/03/06/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8F%91%E5%B1%95%E4%B8%8E%E6%8F%90%E7%A4%BA%E8%AF%8D%E7%9A%84%E5%85%B3%E7%B3%BB/"/>
    <url>/2025/03/06/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8F%91%E5%B1%95%E4%B8%8E%E6%8F%90%E7%A4%BA%E8%AF%8D%E7%9A%84%E5%85%B3%E7%B3%BB/</url>
    
    <content type="html"><![CDATA[<h1 id="目前大模型应用存在的现状"><a href="#目前大模型应用存在的现状" class="headerlink" title="目前大模型应用存在的现状"></a>目前大模型应用存在的现状</h1><ol><li><strong>模型无法进行实时增量训练</strong>：目前，预训练完成后，再微调还是 RLHF（Reinforcement Learning fromHuman Feedback，人类反馈强化学习）都无法让模型学到新的知识，模型参数无法动态更新。</li><li><strong>上下文长度限制</strong>：这与Transformer架构有很大关系，模型上下文长度限制了token的吞吐量，即使RAG在不断推陈出新，也不能解决这一核心问题。模型需要建立更加高效的短期记忆与长期记忆的范式。</li><li><strong>Prompt的必要性</strong>：Prompt的存在核心是希望模型可以按照 COT（Chain of Thought，思维链）的方式，约束它的输出。如果模型具备逻辑推理能力，未来可能不再需要专门的Prompt来约束输出，模型会自动匹配寻找局部最优解的路线。</li></ol><h1 id="什么是提示词？"><a href="#什么是提示词？" class="headerlink" title="什么是提示词？"></a>什么是提示词？</h1><h2 id="1-Prompt的作用"><a href="#1-Prompt的作用" class="headerlink" title="1. Prompt的作用"></a>1. Prompt的作用</h2><p>当前，在大多数语言模型（如 GPT 系列）中，Prompt（提示）是用来引导模型生成特定格式或内容的输入。这些提示通常通过指示任务的方式帮助模型理解问题的背景，并输出符合预期的答案。比如，对于一个问答任务，用户可能会给出一个类似“请回答以下问题：‘X是什么？’”的提示，来确保模型按照问答的格式给出回答。</p><p>COT（Chain of Thought，思维链）是一种常见的 Prompt 形式，它鼓励模型展示推理过程，而不仅仅是直接给出答案。通过这种方式，模型不仅能给出最终结果，还能展示它是如何推导出这个结果的。这样做的目的是提高模型的推理能力，使它能够处理更复杂的任务。</p><h2 id="2-逻辑推理能力和-Prompt-的关系"><a href="#2-逻辑推理能力和-Prompt-的关系" class="headerlink" title="2. 逻辑推理能力和 Prompt 的关系"></a>2. 逻辑推理能力和 Prompt 的关系</h2><p>当前的语言模型大多是通过统计和概率推测来生成回答，它们没有内在的推理能力，而是依赖外部的 Prompt 来指引输出的方向。比如，当用户希望模型按某种特定方式推理时，用户就需要提供提示词，或者明确指出需要展示推理过程。Prompt 就是帮助模型约束和规范化输出的工具。</p><p>然而，未来如果模型具备了更强的内在推理能力，那么它们可能会在没有显式的提示和约束下，自动理解任务要求，并根据上下文和目标自我推理出最佳的输出路径。这意味着，模型在面对复杂问题时能够像人类一样，根据所给的条件和任务要求，自动推测出一个合理的推理链，进而给出答案，而不需要人为提供像“请给出推理过程”这样的提示。</p><h2 id="3-模型自动匹配最优解的假设"><a href="#3-模型自动匹配最优解的假设" class="headerlink" title="3. 模型自动匹配最优解的假设"></a>3. 模型自动匹配最优解的假设</h2><p>随着技术的进步，未来的模型可能会变得足够强大，能够“理解”任务本身的结构，而不再需要通过显式的 Prompt 来进行约束。在这种情况下，模型可以在面对问题时：</p><ul><li>自动识别需要的推理步骤。</li><li>根据任务的要求自我调整输出的方式，寻找最优解。</li><li>自动识别并展示思维链，而不需要依赖外部的 Prompt 来明确要求。</li></ul><p>这种模型不再依赖于用户给出的明确提示，而是能够根据问题的性质自动调整推理过程和输出形式。例如，如果给定了一个数学题，模型不再需要特别的“请展示推理过程”提示，而是能够自然地先进行推理，再给出答案。</p><h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4. 总结"></a>4. 总结</h2><ul><li>现在的模型依赖于Prompt来引导输出，特别是在复杂任务中需要指定推理过程（如COT）来增强推理能力。</li><li>未来的模型，如果具备更强的推理能力，可能会“自动”根据任务自我调整推理过程，无需依赖专门的提示。它们能像人类一样理解任务要求并自行推理，从而减少对人工约束（Prompt）的依赖。</li><li>这意味着，Prompt作为一种约束工具的必要性在未来可能会减弱，特别是在模型自我推理能力更强的情况下，模型将能够更灵活地处理各种任务。</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>Prompt</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>为什么要选择AI智能客服</title>
    <link href="/2025/03/06/%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E9%80%89%E6%8B%A9AI%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/"/>
    <url>/2025/03/06/%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E9%80%89%E6%8B%A9AI%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D/</url>
    
    <content type="html"><![CDATA[<h1 id="为什么要选择AI智能客服？对比传统的深度学习智能客服有什么优势？"><a href="#为什么要选择AI智能客服？对比传统的深度学习智能客服有什么优势？" class="headerlink" title="为什么要选择AI智能客服？对比传统的深度学习智能客服有什么优势？"></a>为什么要选择AI智能客服？对比传统的深度学习智能客服有什么优势？</h1><h2 id="1-实时响应与自动化服务"><a href="#1-实时响应与自动化服务" class="headerlink" title="1. 实时响应与自动化服务"></a>1. 实时响应与自动化服务</h2><p>传统客服系统通常依赖人工处理，当回答的准确度不够时，都会介入人工客服进行处理。加入了大模型后，这个现状不会改变，但能相对提高准确率，以减少转人工的频率。整合了 SpringAI 后，AI 智能客服可以直接根据用户的提问调用系统后台的查询接口，将查询后的数据投喂给模型后再回答。在校验了用户的身份信息的同时，如果客户提供了足够的信息，AI 智能客服可以直接调用后台系统的接口，简易地代替客户进行业务操作，如：</p><ul><li>预约（预定）</li><li>签到（取号）</li><li>信息确认</li><li>变更预约信息等</li></ul><h2 id="2-自然语言理解和语义匹配"><a href="#2-自然语言理解和语义匹配" class="headerlink" title="2. 自然语言理解和语义匹配"></a>2. 自然语言理解和语义匹配</h2><p>采用 SpringAI 和向量模型的结合，不依赖传统的基于规则的自然语言处理（NLP），能够通过语义匹配技术（而不是单纯的关键词匹配）来提高搜索和检索效率，即使是更复杂的、多变的问题也能有效处理。基于向量表示的语义搜索，使得系统可以理解词语的上下文关系，准确理解用户意图。这种方式能够通过“理解”，而非基于传统客服的“简单匹配”来回答用户的问题，即使在没有直接匹配的情况下，也能给出准确的回答。</p><h2 id="3-知识库的更新与推理"><a href="#3-知识库的更新与推理" class="headerlink" title="3. 知识库的更新与推理"></a>3. 知识库的更新与推理</h2><p>知识库通常由人工维护，内容更新不及时，缺乏自动学习和推理的能力，且更新周期长，常常难以跟上快速变化的需求和行业动态，这是行业的通病。想要保证知识库的质量，免不了需要人工干预和微调。虽大模型拥有自学习的能力，但在无法保证数据质量的前提下，当有新的问题或新业务需求出现时，并不建议系统通过向量检索动态学习相关知识，而是记录知识并开发知识学习子模块进行优化。但对比传统的客服系统，在用户问到知识库相关性不大的问题时，系统还可以进行智能推理，根据用户查询的内容自动推测相关信息，提升服务的精度和效率。</p><h2 id="4-多语种跨语义支持"><a href="#4-多语种跨语义支持" class="headerlink" title="4. 多语种跨语义支持"></a>4. 多语种跨语义支持</h2><p>传统的智能客服系统，若支持多语种，往往只能对知识库的查询结果进行简单翻译，常常词不达意，且语种的扩展需要额外的人工配置和开发。通过基于向量的架构，可以轻松实现多语种的支持，模型本身能够理解不同语种之间的语义关系，且不需要人工配置不同语种的规则，可以快速扩展到多个语言或领域，实现跨语言的智能客服。</p><h2 id="5-减少资源浪费与运营成本"><a href="#5-减少资源浪费与运营成本" class="headerlink" title="5. 减少资源浪费与运营成本"></a>5. 减少资源浪费与运营成本</h2><p>AI 智能客服的语义理解能力得到强化后，虽然降低了人工干预的可能，减少了对人工客服的依赖，但做不到完全取代人工客服。我们可以考虑通过结合 SpringAI 的自动化功能，提出能够提高客服效率的方案。传统的业务办理流程都是先验证用户身份信息，确认办理业务，并在业务系统中通过该业务相关的不同页面在电脑上进行操作完成办理。结合 SpringAI 后，可以根据客户提供的信息结合需要办理的业务场景，为人工客服提供跳转到需要办理相关业务链接的按钮（避免模型偶尔理解不够准确，并不进行直接跳转，而是给定相关的多个页面选项），同时在客服页面中能提取到用户基于办理本业务可能要在系统中获取的简易信息汇总。例如：</p><ul><li>如客户需要改签到明天的中午，则提供：<ul><li>客户基础身份信息</li><li>客户已绑定待改签的订单</li><li>需要改签的新时间段的预定情况等</li></ul></li></ul><p>SpringAI 会允许 AI 调用相关接口进行信息查询并汇总，并推荐业务系统的改签页面跳转按钮给业务员进行操作，从而提高客服的业务办理效率。</p><hr>]]></content>
    
    
    
    <tags>
      
      <tag>SpringAI</tag>
      
      <tag>AI智能客服</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>什么是RAG</title>
    <link href="/2025/03/05/%E4%BB%80%E4%B9%88%E6%98%AFRAG/"/>
    <url>/2025/03/05/%E4%BB%80%E4%B9%88%E6%98%AFRAG/</url>
    
    <content type="html"><![CDATA[<h1 id="什么是-RAG？"><a href="#什么是-RAG？" class="headerlink" title="什么是 RAG？"></a>什么是 RAG？</h1><p>RAG（Retrieval Augmented Generation）是一种利用额外信息增强语言模型生成能力的技术。其工作流程如下：</p><ol><li><strong>切分</strong>：将大型文档分割成较小的部分。</li><li><strong>向量化</strong>：将切分后的文本片段转换为向量表示。</li><li><strong>检索</strong>：将输入查询与向量数据库中的向量进行匹配，找到最相似的文本片段。</li><li><strong>生成</strong>：将检索到的相关信息与查询一起传递给大型语言模型（LLM），生成最终的响应。</li></ol><h2 id="RAG应用工作流程"><a href="#RAG应用工作流程" class="headerlink" title="RAG应用工作流程"></a>RAG应用工作流程</h2><p>以下是典型的RAG应用工作流程：</p><p><img src="/img/rag/RAG%E5%BA%94%E7%94%A8%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.jpg" alt="RAG应用工作流程（来自Avi Chawla）"></p><p>RAG通过将额外信息存储为向量，将传入的查询与这些向量匹配，并将最相似的信息与查询一起传递给LLM。</p><p>由于额外的文档可能非常大，第一步需要进行切分操作，将大文档分割成较小、易于管理的部分。这一步至关重要，确保文本能够适应嵌入模型的输入大小，提高检索步骤的效率和准确性，直接影响生成响应的质量。</p><p>这一步至关重要，它确保文本能够适应嵌入模型的输入大小。此外，它提高了检索步骤的效率和准确性，这直接影响生成响应的质量。</p><p><img src="/img/rag/Document-Chunking.jpg" alt="RAG切分策略"></p><h2 id="RAG的5种切分策略"><a href="#RAG的5种切分策略" class="headerlink" title="RAG的5种切分策略"></a>RAG的5种切分策略</h2><p><img src="/img/rag/Chunking-Strategies-For-RAG.jpg" alt="RAG的五种切分策略"></p><h3 id="01-固定大小切分"><a href="#01-固定大小切分" class="headerlink" title="01 固定大小切分"></a>01 固定大小切分</h3><p>最直观的切分方法是根据预定的字符数、单词数或Token数量将文本均匀分割成若干段落。</p><p><img src="/img/rag/%E5%9B%BA%E5%AE%9A%E5%A4%A7%E5%B0%8F%E5%88%87%E5%88%86.jpg" alt="固定大小切分"></p><p>由于直接切分可能会破坏语义流畅性，建议在连续段落间保留一些重叠部分。这种方法易于实现，所有段落大小相同，有助于简化批处理。但它可能在句子或想法中途切分，导致重要信息分散在不同段落中。</p><h3 id="02-语义切分"><a href="#02-语义切分" class="headerlink" title="02 语义切分"></a>02 语义切分</h3><p>根据句子、段落或主题部分等有意义的单元来切分文档。</p><p><img src="/img/rag/%E8%AF%AD%E4%B9%89%E5%88%87%E5%88%86.jpg" alt="语义切分"></p><p>具体方法如下：</p><ul><li>根据句子、段落或主题部分等有意义的单元来切分文档。</li><li>为每个段落生成嵌入。</li><li>假设从第一个段落及其嵌入开始。</li><li>如果第一个段落的嵌入与第二个段落的嵌入余弦相似度较高，则将两个段落组成一个切片。</li><li>持续此过程，直到余弦相似度显著下降。</li><li>一旦下降，开始一个新切片并重复此过程。</li></ul><p>输出可能如下所示：<br><img src="/img/rag/%E8%AF%AD%E4%B9%89%E5%88%87%E5%88%86%E7%BB%93%E6%9E%9C.jpg" alt="语义切分结果"></p><p>这种方式与固定大小切片不同，能够保持语言的自然流畅性，并保留完整的思想。</p><p>由于每个切片语义更为丰富，它提高了检索准确度，进而使LLM生成的响应更加连贯且相关。</p><p>一个小问题是，确定余弦相似度下降的阈值在不同文档间可能有所不同。</p><h3 id="03-递归切分"><a href="#03-递归切分" class="headerlink" title="03 递归切分"></a>03 递归切分</h3><p>首先，基于内在的分隔符（如段落或章节）进行切分。</p><p>然后，如果某个切片的大小超过预定义的切片大小限制，就将其进一步分割。如果切片符合大小限制，则不再进行切分。</p><p><img src="/img/rag/%E9%80%92%E5%BD%92%E5%88%87%E5%88%86.jpg" alt="递归切分"></p><p>输出结果可能如下所示：<br><img src="/img/rag/%E9%80%92%E5%BD%92%E5%88%87%E5%88%86%E7%BB%93%E6%9E%9C.jpg" alt="递归切分结果"></p><p>如上所示：</p><ul><li><p>首先，我们定义了两个切片（紫色的两个段落）。</p></li><li><p>接下来，第1段被进一步分割成较小的切片。</p></li></ul><p>与固定大小的切片不同，这种方法也保持了语言的自然流畅性，并保留了完整的思想。</p><p>不过，在实现和计算复杂性方面有一些额外的开销。</p><h3 id="04-基于文档结构的切分"><a href="#04-基于文档结构的切分" class="headerlink" title="04 基于文档结构的切分"></a>04 基于文档结构的切分</h3><p>这是一种直观的方法。</p><p><img src="/img/rag/%E5%9F%BA%E4%BA%8E%E6%96%87%E6%A1%A3%E7%BB%93%E6%9E%84%E7%9A%84%E5%88%87%E5%88%86.jpg" alt="基于文档结构的切分"></p><p>利用文档内在的结构（如标题、章节或段落）定义切片边界。</p><p>这种方式能保持文档的结构完整性，确保切片与文档的逻辑部分对齐。</p><p>该方法假设文档结构清晰，但这可能并非总是如此。此外，切片长度可能不同，甚至超过模型的Token限制。可以尝试与递归切分结合使用。</p><p>输出结果可能如下所示：</p><p><img src="/img/rag/%E5%9F%BA%E4%BA%8E%E6%96%87%E6%A1%A3%E7%BB%93%E6%9E%84%E7%9A%84%E5%88%87%E5%88%86%E7%BB%93%E6%9E%9C.jpg" alt="基于文档结构的切分结果"></p><h3 id="05-基于LLM的切分"><a href="#05-基于LLM的切分" class="headerlink" title="05 基于LLM的切分"></a>05 基于LLM的切分</h3><p>LLM可以通过提示词生成语义隔离且有意义的切片。</p><p><img src="/img/rag/%E5%9F%BA%E4%BA%8ELLM%E7%9A%84%E5%88%87%E5%88%86.jpg" alt="基于LLM的切分"></p><p>这种方法确保了高语义准确性，因为LLM能理解上下文和意义，远超简单的启发式方法。</p><p>唯一的问题是，这种方式的计算成本是五种方法中最高的。</p><p>此外，由于LLM通常有上下文窗口限制，需要对此加以处理。</p><p>每种技术都有各自的优劣。语义切分在许多情况下效果不错，但仍需要测试。</p><p>最终的选择将取决于内容的性质、嵌入模型的能力和计算资源等。</p>]]></content>
    
    
    
    <tags>
      
      <tag>RAG</tag>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大模型的常用部署方式</title>
    <link href="/2025/03/05/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%B8%B8%E7%94%A8%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F/"/>
    <url>/2025/03/05/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%B8%B8%E7%94%A8%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F/</url>
    
    <content type="html"><![CDATA[<h1 id="常用的大模型部署方式"><a href="#常用的大模型部署方式" class="headerlink" title="常用的大模型部署方式"></a>常用的大模型部署方式</h1><p>随着人工智能技术的飞速发展，大型语言模型（LLM）在自然语言处理、机器翻译、文本生成等领域取得了显著成果。本文将探讨当前AI大模型的现状，介绍几种常见的大模型部署方式，并对比它们的优劣和适用场景。</p><h2 id="1-AI大模型的现状"><a href="#1-AI大模型的现状" class="headerlink" title="1. AI大模型的现状"></a>1. AI大模型的现状</h2><p>近年来，AI大模型取得了巨大的进展。当前，人工智能领域的研究主要集中在人工狭义智能（ANI）和人工通用智能（AGI）上，而大模型被视为实现AGI的关键技术。通过在海量数据上进行大规模训练，大模型可以处理多种下游任务。然而，随着模型规模的扩大，也带来了计算资源消耗、数据隐私问题及模型偏见等挑战。</p><h2 id="2-常见的大模型选项"><a href="#2-常见的大模型选项" class="headerlink" title="2. 常见的大模型选项"></a>2. 常见的大模型选项</h2><p>目前市面上常见的大型语言模型包括：</p><ul><li><strong>GPT系列</strong>：由OpenAI开发，具有强大的文本生成和理解能力，广泛应用于各种NLP任务。</li><li><strong>BERT</strong>：由Google提出，采用双向编码器表示（Bidirectional Encoder Representations），擅长各种上下文理解任务。</li><li><strong>LLaMA</strong>：Meta发布的开源模型，性能接近GPT系列，但资源消耗较低，适用于多种应用场景。</li><li><strong>DeepSeek</strong>：国内开发的大模型，具有与GPT相当的性能，同时成本更具竞争力。</li></ul><p>选择合适的大模型需根据具体需求，如任务类型、预算、计算资源等进行权衡。</p><h2 id="3-比较部署方式"><a href="#3-比较部署方式" class="headerlink" title="3. 比较部署方式"></a>3. 比较部署方式</h2><p>在选择部署方式时，我们将对比 <strong>Python环境运行大模型</strong>、<strong>Ollama运行大模型</strong> 和 <strong>使用开源平台部署大模型</strong> 三种方式，分析它们的优缺点以及适用场景。</p><h3 id="Python环境运行大模型"><a href="#Python环境运行大模型" class="headerlink" title="Python环境运行大模型"></a>Python环境运行大模型</h3><p>Python是AI领域的主流编程语言，拥有丰富的深度学习框架，如TensorFlow、PyTorch等。通过这些框架，用户可以在Python环境下运行大模型。此方式通常需要强大的计算资源，尤其是高性能的GPU支持。</p><p><strong>优点：</strong></p><ul><li>灵活性高：可以自由选择和调整模型架构、参数等。</li><li>兼容性强：支持多种深度学习框架，能够灵活与其他技术结合。</li><li>丰富的生态系统：Python有着强大的开发者社区和工具支持。</li></ul><p><strong>缺点：</strong></p><ul><li>高资源需求：需要强大的硬件支持，尤其是GPU&#x2F;TPU。</li><li>复杂性：配置和调优过程繁琐，尤其在大规模部署时需要更多专业知识。</li><li>数据隐私问题：数据需要通过网络传输，存在泄露风险。</li></ul><p><strong>适用场景：</strong></p><ul><li>需要高度自定义和灵活配置的场景。</li><li>具备强大计算资源的团队或企业。</li></ul><h3 id="Ollama运行大模型"><a href="#Ollama运行大模型" class="headerlink" title="Ollama运行大模型"></a>Ollama运行大模型</h3><p>Ollama是一个轻量级的大模型推理框架，支持本地部署并确保数据隐私。它专注于简化本地推理部署过程，特别适合对数据隐私要求较高的场景。</p><p><strong>优点：</strong></p><ul><li><strong>数据隐私保护</strong>：数据仅在本地处理，无需上传至云端。</li><li><strong>低延迟</strong>：本地部署能减少网络传输带来的延迟。</li><li><strong>易用性</strong>：部署流程简单，适合开发者快速上手。</li></ul><p><strong>缺点：</strong></p><ul><li><strong>资源限制</strong>：本地部署仍然依赖于硬件资源，尤其是GPU和内存。</li><li><strong>扩展性较差</strong>：对于需要大规模处理的场景，单机部署可能会面临瓶颈。</li><li><strong>功能有限</strong>：相比Python框架，Ollama在灵活性和扩展性上可能不够强大。</li></ul><p><strong>适用场景：</strong></p><ul><li>对数据隐私要求较高，且不需要极端计算能力的场景。</li><li>小型企业或开发者进行快速实验和部署。</li></ul><h3 id="使用开源平台部署大模型"><a href="#使用开源平台部署大模型" class="headerlink" title="使用开源平台部署大模型"></a>使用开源平台部署大模型</h3><p>除了Python环境和Ollama，一些开源平台如 <strong>LM Studio</strong> 和 <strong>Dify</strong> 也为大模型提供了简单的部署方案。开源平台通常提供现成的配置和预训练模型，开发者可以轻松进行部署和管理。</p><p><strong>优点：</strong></p><ul><li><strong>简化部署</strong>：通过现成的工具和配置，开发者无需过多关注底层实现。</li><li><strong>社区支持</strong>：开源平台有活跃的社区支持，提供大量的教程和资源。</li><li><strong>快速启动</strong>：可以快速启动大模型，尤其适合快速迭代和测试。</li></ul><p><strong>缺点：</strong></p><ul><li><strong>功能受限</strong>：开源平台可能对模型的自定义性和扩展性有所限制。</li><li><strong>依赖社区更新</strong>：更新和维护可能受到社区驱动，存在不稳定性。</li><li><strong>数据隐私</strong>：某些开源平台可能需要将数据上传至云端，存在数据泄露风险。</li></ul><p><strong>适用场景：</strong></p><ul><li>中小型团队或个人开发者，快速搭建和验证大模型的场景。</li><li>开源社区支持的企业，愿意以较低成本进行大模型实验。</li></ul><h2 id="4-部署方式对比总结"><a href="#4-部署方式对比总结" class="headerlink" title="4. 部署方式对比总结"></a>4. 部署方式对比总结</h2><table><thead><tr><th>部署方式</th><th>优点</th><th>缺点</th><th>适用场景</th></tr></thead><tbody><tr><td><strong>Python环境</strong></td><td>高度灵活，支持多种框架，生态丰富</td><td>高资源需求，配置复杂，数据隐私问题</td><td>需要自定义和灵活配置的场景，计算资源丰富的企业</td></tr><tr><td><strong>Ollama</strong></td><td>数据隐私保护，低延迟，部署简单</td><td>资源限制，扩展性较差，功能相对有限</td><td>对数据隐私要求高，小型企业或快速部署的场景</td></tr><tr><td><strong>开源平台（LM Studio、Dify）</strong></td><td>简化部署，社区支持，快速启动</td><td>功能受限，依赖社区更新，数据隐私问题</td><td>中小型团队，快速实验和验证的场景</td></tr></tbody></table><h2 id="5-结论"><a href="#5-结论" class="headerlink" title="5. 结论"></a>5. 结论</h2><p>选择合适的大模型部署方式需要综合考虑多个因素，包括计算资源、数据隐私、维护难度和灵活性等。如果需要高度定制化的模型和灵活性，<strong>Python环境</strong>是最佳选择；如果数据隐私和低延迟是首要考虑，且需要简单易用的解决方案，<strong>Ollama</strong>可以是理想的选择；而对于快速部署、验证和开发，<strong>开源平台</strong>则提供了一个低成本且高效的解决方案。每种方式都有其特定的优缺点，适合不同规模和需求的应用场景。</p>]]></content>
    
    
    
    <tags>
      
      <tag>RAG</tag>
      
      <tag>Ollama</tag>
      
      <tag>DeepSeek</tag>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
